import os
import streamlit as st
import pandas as pd
import json
from openai import OpenAI
import sqlite3
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import threading
from contextlib import contextmanager
import re
import requests
from io import BytesIO

# åˆå§‹çš„å“ç‰Œå’Œå‹å·æ˜ å°„ï¼ˆå°†ä½œä¸ºå¤‡é€‰ï¼‰
INITIAL_BRAND_MODEL_MAPPING = {
    "å¹¿å·æ•°æ§": {
        "æ•°æ§ç³»ç»Ÿ": ["GSK 980T", "GSK 928TA", "GSK 928TC"],
        "é©±åŠ¨å™¨": ["DA98"]
    },
    "ä¸‰è±": {
        "å˜é¢‘å™¨": ["ä¸‰è±FR-E700"]
    }
}

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ•°æ§è®¾å¤‡æ•…éšœè¯Šæ–­ç³»ç»Ÿ",
    page_icon="ğŸ”§",
    layout="wide"
)

# åˆ›å»ºçº¿ç¨‹æœ¬åœ°å­˜å‚¨
thread_local = threading.local()

def normalize_alarm_code(alarm_code):
    """è§„èŒƒåŒ–æŠ¥è­¦ä»£ç ï¼šå»é™¤å¼€å¤´å¤šä½™çš„0ï¼Œä½†ä¿ç•™å°æ•°ç‚¹å’Œå°æ•°éƒ¨åˆ†"""
    if pd.isna(alarm_code) or alarm_code == '':
        return ''
    
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶å»é™¤ç©ºæ ¼
    alarm_str = str(alarm_code).strip()
    
    # å¦‚æœåŒ…å«å°æ•°ç‚¹ï¼Œå¤„ç†å°æ•°éƒ¨åˆ†
    if '.' in alarm_str:
        # åˆ†å‰²æ•´æ•°éƒ¨åˆ†å’Œå°æ•°éƒ¨åˆ†
        int_part, dec_part = alarm_str.split('.', 1)
        # å»é™¤æ•´æ•°éƒ¨åˆ†å¼€å¤´çš„0
        int_part = int_part.lstrip('0') or '0'
        # é‡æ–°ç»„åˆ
        return f"{int_part}.{dec_part}"
    else:
        # å»é™¤å¼€å¤´çš„0ï¼Œä½†å¦‚æœå…¨éƒ¨æ˜¯0åˆ™ä¿ç•™ä¸€ä¸ª0
        return alarm_str.lstrip('0') or '0'

def extract_brand_model_mapping(df):
    """ä»æ•°æ®æ¡†ä¸­æå–å“ç‰Œå’Œå‹å·çš„æ˜ å°„å…³ç³»"""
    brand_model_mapping = {}
    
    # æ£€æŸ¥æ•°æ®æ¡†ä¸­æ˜¯å¦æœ‰å“ç‰Œå’Œäº§å“ç±»å‹åˆ—
    has_brand_column = 'å“ç‰Œ' in df.columns
    has_product_type_column = 'äº§å“ç±»å‹' in df.columns
    has_model_column = 'å‹å·' in df.columns
    
    if has_brand_column and has_product_type_column and has_model_column:
        # æŒ‰å“ç‰Œå’Œäº§å“ç±»å‹åˆ†ç»„ï¼Œæ”¶é›†å‹å·
        grouped = df.groupby(['å“ç‰Œ', 'äº§å“ç±»å‹'])['å‹å·'].unique()
        
        for (brand, product_type), models in grouped.items():
            if pd.notna(brand) and pd.notna(product_type) and brand != 'æœªçŸ¥' and product_type != 'æœªçŸ¥':
                if brand not in brand_model_mapping:
                    brand_model_mapping[brand] = {}
                
                # è¿‡æ»¤æ‰ç©ºå€¼å’ŒæœªçŸ¥å€¼
                valid_models = [model for model in models if pd.notna(model) and model != 'æœªçŸ¥']
                if valid_models:
                    brand_model_mapping[brand][product_type] = valid_models
    
    return brand_model_mapping

class RobustDataLoader:
    """å¥å£®çš„æ•°æ®åŠ è½½å™¨ï¼Œå¤„ç†å¤šå­è¡¨Excelæ–‡ä»¶"""
    
    def __init__(self):
        self.df = None
        self.data_loaded = False
        self.error_message = None  # æ·»åŠ é”™è¯¯ä¿¡æ¯å±æ€§
    
    def load_from_excel(self, file_obj):
        """ä»Excelæ–‡ä»¶åŠ è½½æ•°æ®"""
        try:
            # é‡ç½®åŠ è½½çŠ¶æ€
            self.data_loaded = False
            self.error_message = None  # é‡ç½®é”™è¯¯ä¿¡æ¯

            # å°è¯•ç¡®å®šæ–‡ä»¶ç±»å‹å¹¶é€‰æ‹©åˆé€‚çš„å¼•æ“
            if hasattr(file_obj, 'name'):
                file_name = file_obj.name.lower()
                if file_name.endswith('.xlsx'):
                    engine = 'openpyxl'
                elif file_name.endswith('.xls'):
                    engine = 'xlrd'
                else:
                    # é»˜è®¤ä½¿ç”¨openpyxl
                    engine = 'openpyxl'
            else:
                # å¯¹äºæ²¡æœ‰æ–‡ä»¶åçš„å¯¹è±¡ï¼ˆå¦‚BytesIOï¼‰ï¼Œå°è¯•ä¸¤ç§å¼•æ“
                engine = None
            
            # è¯»å–Excelæ–‡ä»¶
            if engine:
                excel_file = pd.ExcelFile(file_obj, engine=engine)
            else:
                # å°è¯•è‡ªåŠ¨æ£€æµ‹å¼•æ“
                try:
                    excel_file = pd.ExcelFile(file_obj, engine='openpyxl')
                except:
                    excel_file = pd.ExcelFile(file_obj, engine='xlrd')
            
            sheet_names = excel_file.sheet_names
            
            # è¯»å–æ‰€æœ‰å­è¡¨å¹¶åˆå¹¶
            all_sheets = []
            for sheet_name in sheet_names:
                try:
                    # è¯»å–æ—¶ä¸è½¬æ¢æ•°æ®ç±»å‹ï¼Œä¿æŒåŸå§‹æ ¼å¼
                    if engine:
                        sheet_df = pd.read_excel(file_obj, sheet_name=sheet_name, dtype=str, engine=engine)
                    else:
                        # å°è¯•è‡ªåŠ¨æ£€æµ‹å¼•æ“
                        try:
                            sheet_df = pd.read_excel(file_obj, sheet_name=sheet_name, dtype=str, engine='openpyxl')
                        except:
                            sheet_df = pd.read_excel(file_obj, sheet_name=sheet_name, dtype=str, engine='xlrd')
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºè¡¨
                    if sheet_df.empty:
                        st.warning(f"å·¥ä½œè¡¨ '{sheet_name}' ä¸ºç©ºï¼Œå·²è·³è¿‡")
                        continue
                        
                    # æ·»åŠ å­è¡¨åç§°ä½œä¸ºæ–°åˆ—
                    sheet_df['æ¥æºå­è¡¨'] = sheet_name
                    
                    # æ ‡å‡†åŒ–åˆ—åï¼ˆå»é™¤å‰åç©ºæ ¼ï¼‰
                    sheet_df.columns = sheet_df.columns.str.strip()
                    
                    all_sheets.append(sheet_df)
                except Exception as e:
                    st.warning(f"è¯»å–å·¥ä½œè¡¨ '{sheet_name}' æ—¶å‡ºé”™: {e}ï¼Œå·²è·³è¿‡")
            
            if not all_sheets:
                self.error_message = "æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•å·¥ä½œè¡¨ï¼Œè¯·æ£€æŸ¥Excelæ–‡ä»¶æ ¼å¼"
                st.error(self.error_message)
                return False
            
            # åˆå¹¶æ‰€æœ‰å­è¡¨
            # æ‰¾å‡ºæ‰€æœ‰å¯èƒ½çš„åˆ—
            all_columns = set()
            for sheet in all_sheets:
                all_columns.update(sheet.columns)
            
            # ç¡®ä¿æ‰€æœ‰DataFrameæœ‰ç›¸åŒçš„åˆ—
            standardized_sheets = []
            for sheet in all_sheets:
                # æ·»åŠ ç¼ºå¤±çš„åˆ—
                for col in all_columns:
                    if col not in sheet.columns:
                        sheet[col] = None
                # é‡æ–°æ’åºåˆ—
                sheet = sheet[list(all_columns)]
                standardized_sheets.append(sheet)
            
            # åˆå¹¶æ‰€æœ‰å­è¡¨
            self.df = pd.concat(standardized_sheets, ignore_index=True)
            
            # æ ‡å‡†åŒ–åˆ—åï¼ˆä¸­æ–‡ï¼‰
            column_mapping = {
                'æŠ¥è­¦ä»£ç ': 'æŠ¥è­¦ä»£ç ',
                'æ•…éšœç°è±¡': 'æ•…éšœç°è±¡',
                'åŸå› ': 'åŸå› ',
                'å¤„ç†æ–¹æ³•': 'å¤„ç†æ–¹æ³•',
                'æ•…éšœç±»å‹': 'æ•…éšœç±»å‹',
                'å‹å·': 'å‹å·',
                'å“ç‰Œ': 'å“ç‰Œ',
                'äº§å“ç±»å‹': 'äº§å“ç±»å‹',
                'æ¥æºå­è¡¨': 'æ¥æºå­è¡¨'
            }
            
            # é‡å‘½ååˆ—
            for old_col, new_col in column_mapping.items():
                if old_col in self.df.columns:
                    self.df.rename(columns={old_col: new_col}, inplace=True)
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = ['æŠ¥è­¦ä»£ç ', 'æ•…éšœç°è±¡', 'åŸå› ', 'å¤„ç†æ–¹æ³•', 'æ•…éšœç±»å‹', 'å‹å·']
            missing_columns = [col for col in required_columns if col not in self.df.columns]
            
            if missing_columns:
                self.error_message = f"Excelæ–‡ä»¶ä¸­ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}"
                st.error(self.error_message)
                return False
            
            # å…³é”®ä¿®æ”¹ï¼šç¡®ä¿æ‰€æœ‰åˆ—çš„æ•°æ®ç±»å‹ä¸€è‡´
            # å…ˆå¡«å……ç©ºå€¼
            self.df = self.df.fillna('æœªçŸ¥')

            if 'å‹å·' in self.df.columns:
                self.df['å‹å·'] = self.df['å‹å·'].astype(str).str.strip()
            
            # å…³é”®ä¿®æ”¹ï¼šä¿ç•™æŠ¥è­¦ä»£ç çš„åŸå§‹æ ¼å¼ï¼Œä½†æ·»åŠ è§„èŒƒåŒ–ç‰ˆæœ¬ç”¨äºæ¯”è¾ƒ
            if 'æŠ¥è­¦ä»£ç ' in self.df.columns:
                # ä¿ç•™åŸå§‹æŠ¥è­¦ä»£ç 
                self.df['æŠ¥è­¦ä»£ç _åŸå§‹'] = self.df['æŠ¥è­¦ä»£ç ']
                # åˆ›å»ºè§„èŒƒåŒ–ç‰ˆæœ¬ç”¨äºæ¯”è¾ƒ
                self.df['æŠ¥è­¦ä»£ç _è§„èŒƒåŒ–'] = self.df['æŠ¥è­¦ä»£ç '].apply(normalize_alarm_code)
            
            # æ•°æ®åŠ è½½æˆåŠŸåï¼Œè®¾ç½®åŠ è½½çŠ¶æ€
            self.data_loaded = True
            
            return True
            
        except Exception as e:
            self.data_loaded = False
            self.error_message = f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}"
            st.error(self.error_message)
            return False

    def search_by_pandas(self, model=None, alarm_code=None, limit=1000):
        """ä½¿ç”¨pandasè¿›è¡Œç²¾ç¡®æŸ¥è¯¢"""
        if not self.data_loaded or self.df is None:
            return pd.DataFrame()
        
        try:
            # æ„å»ºç­›é€‰æ¡ä»¶
            condition = pd.Series([True] * len(self.df))
            
            if model and model != "è¯·é€‰æ‹©":
                condition = condition & (self.df['å‹å·'] == model)
            
            if alarm_code:
                # è§„èŒƒåŒ–ç”¨æˆ·è¾“å…¥çš„æŠ¥è­¦ä»£ç 
                normalized_alarm_code = normalize_alarm_code(alarm_code)
                # ä½¿ç”¨è§„èŒƒåŒ–ç‰ˆæœ¬è¿›è¡Œæ¯”è¾ƒ
                condition = condition & (self.df['æŠ¥è­¦ä»£ç _è§„èŒƒåŒ–'] == normalized_alarm_code)
            
            # åº”ç”¨ç­›é€‰æ¡ä»¶
            result = self.df[condition].copy()
            
            # é™åˆ¶ç»“æœæ•°é‡
            if len(result) > limit:
                result = result.head(limit)
            
            return result
        except Exception as e:
            st.error(f"PandasæŸ¥è¯¢é”™è¯¯: {e}")
            return pd.DataFrame()

class SemanticSearcher:
    """è¯­ä¹‰æœç´¢ç±»"""
    def __init__(self, df):
        # å¤åˆ¶æ•°æ®
        self.df = df.copy()
        self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

        def clean_text(x):
            if pd.isna(x): return ""
            s = str(x).strip()
            return "" if s.lower() in ["æœªçŸ¥", "nan", "none", ""] else s

        # é¢„è®¡ç®—å‘é‡
        texts_ph = self.df['æ•…éšœç°è±¡'].apply(clean_text).tolist()
        texts_rs = self.df['åŸå› '].apply(clean_text).tolist()

        self.phenomenon_embeddings = self.embedding_model.encode(texts_ph)
        self.reason_embeddings = self.embedding_model.encode(texts_rs)
    
    def semantic_search(self, query_text, candidate_indices=None, top_k=5, min_similarity=0.3):
        """è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢ - åŒæ—¶åœ¨æ•…éšœç°è±¡å’ŒåŸå› ä¸­æœç´¢"""
        if not query_text.strip():
            return []

        query_embedding = self.embedding_model.encode([query_text])

        if candidate_indices is not None:
            # ä½¿ç”¨DataFrameç´¢å¼•è¿›è¡Œç­›é€‰
            candidate_mask = self.df.index.isin(candidate_indices)
            phenomenon_embeddings = self.phenomenon_embeddings[candidate_mask]
            reason_embeddings = self.reason_embeddings[candidate_mask]
            candidate_df = self.df[candidate_mask]
        else:
            phenomenon_embeddings = self.phenomenon_embeddings
            reason_embeddings = self.reason_embeddings
            candidate_df = self.df
        
        # è®¡ç®—ä¸æ•…éšœç°è±¡çš„ç›¸ä¼¼åº¦
        phenomenon_similarities = cosine_similarity(query_embedding, phenomenon_embeddings)[0]
        
        # è®¡ç®—ä¸åŸå› çš„ç›¸ä¼¼åº¦
        reason_similarities = cosine_similarity(query_embedding, reason_embeddings)[0]
        
        # å–ä¸¤è€…ä¸­çš„è¾ƒé«˜å€¼ä½œä¸ºæœ€ç»ˆç›¸ä¼¼åº¦
        similarities = np.maximum(phenomenon_similarities, reason_similarities)
        
        # è·å–TopKç»“æœ
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        top_sims = similarities[top_indices]
    
        results = []
        for idx, sim in zip(top_indices, top_sims):
            if sim < min_similarity:
                continue
            # è·å–åŸå§‹è¡Œ
            if candidate_indices is not None:
                # å¦‚æœæœ‰é™å®šå€™é€‰é›†ï¼Œéœ€è¦æ˜ å°„å›åŸå§‹ç´¢å¼•
                original_idx = candidate_df.iloc[idx].name
            else:
                original_idx = idx
                
            row = self.df.loc[original_idx]
            results.append({
                'index': int(original_idx),
                'similarity': float(sim),
                'data': row.to_dict()
            })

        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        return results[:top_k]

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []
if "api_key" not in st.session_state:
    st.session_state.api_key = st.secrets.get("DEEPSEEK_API_KEY", "")
if "base_url" not in st.session_state:
    st.session_state.base_url = "https://api.deepseek.com/v1"
if "loader" not in st.session_state:
    st.session_state.loader = RobustDataLoader()
    st.session_state.data_loaded = False
if "searcher" not in st.session_state:
    st.session_state.searcher = None
if "df" not in st.session_state:
    st.session_state.df = None
if "page" not in st.session_state:
    st.session_state.page = "èŠå¤©é¡µé¢"
if "last_uploaded_file" not in st.session_state:
    st.session_state.last_uploaded_file = None
if "current_results" not in st.session_state:
    st.session_state.current_results = []
if "brand_model_mapping" not in st.session_state:
    st.session_state.brand_model_mapping = INITIAL_BRAND_MODEL_MAPPING
if "github_url" not in st.session_state:
    st.session_state.github_url = "https://github.com/lostintime-123/streamlit_test/raw/refs/heads/main/data.xlsx"

# ä¾§è¾¹æ å¯¼èˆª
st.sidebar.title("ğŸ”§ æ•°æ§è®¾å¤‡æ•…éšœè¯Šæ–­ç³»ç»Ÿ")
page = st.sidebar.radio("å¯¼èˆª", ["èŠå¤©é¡µé¢", "æ•°æ®å±•ç¤º", "ä½¿ç”¨è¯´æ˜"])

# APIé…ç½®
st.sidebar.header("âš™ï¸ APIé…ç½®")
api_key = st.sidebar.text_input("å¤§æ¨¡å‹APIå¯†é’¥", value="", type="password")
base_url = st.sidebar.text_input("APIåŸºç¡€URL", value=st.session_state.base_url)

if st.sidebar.button("ä¿å­˜APIé…ç½®"):
    st.session_state.api_key = api_key
    st.session_state.base_url = base_url
    st.sidebar.success("APIé…ç½®å·²ä¿å­˜")

# è®¾å¤‡ç­›é€‰
st.sidebar.header("ğŸ“‹ è®¾å¤‡ç­›é€‰")

# è·å–å“ç‰Œåˆ—è¡¨
brands = list(st.session_state.brand_model_mapping.keys())
selected_brand = st.sidebar.selectbox("é€‰æ‹©å“ç‰Œ", ["è¯·é€‰æ‹©"] + brands, index=0)

# è·å–äº§å“ç±»å‹
product_types = []
if selected_brand != "è¯·é€‰æ‹©":
    product_types = list(st.session_state.brand_model_mapping[selected_brand].keys())
selected_product_type = st.sidebar.selectbox("é€‰æ‹©äº§å“ç±»å‹", ["è¯·é€‰æ‹©"] + product_types, index=0)

# è·å–å‹å·
models = []
if selected_product_type != "è¯·é€‰æ‹©":
    models = st.session_state.brand_model_mapping[selected_brand][selected_product_type]
selected_model = st.sidebar.selectbox("é€‰æ‹©å…·ä½“å‹å·*", ["è¯·é€‰æ‹©"] + models, index=0)

alarm_code = st.sidebar.text_input("æŠ¥è­¦ä»£ç ï¼ˆå¯é€‰ï¼‰", "")

# æ•°æ®ä¸Šä¼ 
st.sidebar.header("ğŸ“¤ æ•°æ®ä¸Šä¼ ")

# ä½¿ç”¨é€‰é¡¹å¡å¸ƒå±€
data_tab1, data_tab2 = st.sidebar.tabs(["ä¸Šä¼ æ–‡ä»¶", "GitHubåœ°å€"])

with data_tab1:
    uploaded_file = st.file_uploader("ä¸Šä¼ æ•…éšœæ•°æ®Excelæ–‡ä»¶", type=["xlsx", "xls"], key="file_uploader")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½æ•°æ®
    if uploaded_file is not None:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„æ–‡ä»¶
        if uploaded_file != st.session_state.last_uploaded_file:
            st.session_state.last_uploaded_file = uploaded_file
            st.session_state.data_loaded = False
            
        if st.button("åŠ è½½æ•°æ®", key="load_uploaded"):
            try:
                if st.session_state.loader.load_from_excel(uploaded_file):
                    st.session_state.df = st.session_state.loader.df
                    st.session_state.data_loaded = True
                    
                    # åˆå§‹åŒ–è¯­ä¹‰æœç´¢å™¨
                    st.session_state.searcher = SemanticSearcher(st.session_state.df)
                    
                    # ä»æ•°æ®ä¸­æå–å“ç‰Œå’Œå‹å·æ˜ å°„
                    extracted_mapping = extract_brand_model_mapping(st.session_state.df)
                    if extracted_mapping:
                        st.session_state.brand_model_mapping = extracted_mapping
                        # st.success(f"æ•°æ®åŠ è½½æˆåŠŸï¼å…± {len(st.session_state.df)} æ¡è®°å½•ï¼Œå·²è‡ªåŠ¨æ›´æ–°å“ç‰Œå’Œå‹å·åˆ—è¡¨")
                    else:
                        st.session_state.brand_model_mapping = INITIAL_BRAND_MODEL_MAPPING
                        # st.success(f"æ•°æ®åŠ è½½æˆåŠŸï¼å…± {len(st.session_state.df)} æ¡è®°å½•ï¼Œä½†æœªèƒ½æå–å“ç‰Œå’Œå‹å·ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„")
                    
                    st.rerun()
                else:
                    st.error("æ•°æ®åŠ è½½å¤±è´¥")
            except Exception as e:
                st.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        
        # æ˜¾ç¤ºæ•°æ®åŠ è½½çŠ¶æ€
        if st.session_state.data_loaded:
            st.success("æ•°æ®å·²åŠ è½½")
        else:
            st.warning("æ•°æ®æœªåŠ è½½ï¼Œè¯·ç‚¹å‡»'åŠ è½½æ•°æ®'æŒ‰é’®")
    else:
        # ä»…å½“ç”¨æˆ·çœŸçš„ç‚¹è¿‡ä¸Šä¼ åŒºåŸŸä½†æ²¡æœ‰æ–‡ä»¶æ—¶ï¼Œæ‰é‡ç½®
        if st.session_state.last_uploaded_file is not None:
            st.session_state.data_loaded = False
            st.session_state.last_uploaded_file = None

with data_tab2:
    github_url = st.text_input(
        "GitHubæ–‡ä»¶åœ°å€", 
        value=st.session_state.github_url,
        placeholder="ä¾‹å¦‚: https://github.com/ç”¨æˆ·å/é¡¹ç›®å/raw/refs/heads/main/æ–‡ä»¶å.xlsx",
        key="github_url_input"
    )
    
    if st.button("ä»GitHubåŠ è½½", key="load_github"):
        if not github_url:
            st.error("è¯·è¾“å…¥GitHubæ–‡ä»¶åœ°å€")
        else:
            try:
                # éªŒè¯URLæ ¼å¼
                if not github_url.startswith(('http://', 'https://')):
                    st.error("è¯·è¾“å…¥æœ‰æ•ˆçš„URLåœ°å€")
                elif 'raw' not in github_url:
                    # å¦‚æœç”¨æˆ·æä¾›äº†æ™®é€šçš„GitHub URLï¼Œå°è¯•è½¬æ¢ä¸ºraw URL
                    st.warning("å»ºè®®ä½¿ç”¨åŒ…å«rawæ ¼å¼çš„URL")
                
                # æ˜¾ç¤ºåŠ è½½è¿›åº¦
                with st.spinner("åŠ è½½GitHubæ–‡ä»¶..."):
                    # æ·»åŠ è¯·æ±‚å¤´æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                    }
                    
                    # å‘é€è¯·æ±‚è·å–æ–‡ä»¶
                    response = requests.get(github_url, headers=headers)
                    
                    # æ£€æŸ¥å“åº”çŠ¶æ€
                    if response.status_code != 200:
                        st.error(f"ä¸‹è½½å¤±è´¥ï¼ŒHTTPçŠ¶æ€ç : {response.status_code}")
                        st.error(f"å“åº”å†…å®¹: {response.text[:200]}...")
                        st.stop()
                    
                    # æ£€æŸ¥å†…å®¹ç±»å‹
                    content_type = response.headers.get('content-type', '')
                    if 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet' not in content_type and \
                       'application/octet-stream' not in content_type:
                        st.warning(f"ä¸‹è½½çš„å†…å®¹ç±»å‹å¯èƒ½ä¸æ˜¯Excelæ–‡ä»¶: {content_type}")
                    
                    # å°†å†…å®¹è½¬æ¢ä¸ºæ–‡ä»¶å¯¹è±¡
                    file_obj = BytesIO(response.content)
                    
                    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                    st.info(f"ä¸‹è½½æˆåŠŸï¼Œæ–‡ä»¶å¤§å°: {len(response.content)} å­—èŠ‚")
                    
                    # åˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®åŠ è½½å™¨å®ä¾‹ï¼Œç¡®ä¿çŠ¶æ€æ­£ç¡®
                    new_loader = RobustDataLoader()
                    
                    # åŠ è½½æ•°æ® - æ·»åŠ è°ƒè¯•ä¿¡æ¯
                    load_success = new_loader.load_from_excel(file_obj)
                    
                    if load_success:
                        # æ›´æ–°ä¼šè¯çŠ¶æ€
                        st.session_state.loader = new_loader
                        st.session_state.df = new_loader.df
                        st.session_state.data_loaded = True
                        st.session_state.github_url = github_url
                        
                        # åˆå§‹åŒ–è¯­ä¹‰æœç´¢å™¨
                        st.session_state.searcher = SemanticSearcher(st.session_state.df)
                        
                        # ä»æ•°æ®ä¸­æå–å“ç‰Œå’Œå‹å·æ˜ å°„
                        extracted_mapping = extract_brand_model_mapping(st.session_state.df)
                        # st.info(f"æå–çš„å“ç‰Œå‹å·æ˜ å°„: {extracted_mapping}")
                        
                        if extracted_mapping:
                            st.session_state.brand_model_mapping = extracted_mapping
                            # ä½¿ç”¨toastæ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
                            # st.info(f"æ•°æ®åŠ è½½æˆåŠŸï¼å…± {len(st.session_state.df)} æ¡è®°å½•ï¼Œå·²è‡ªåŠ¨æ›´æ–°å“ç‰Œå’Œå‹å·åˆ—è¡¨", icon="âœ…")
                        else:
                            st.session_state.brand_model_mapping = INITIAL_BRAND_MODEL_MAPPING
                            # st.info(f"æ•°æ®åŠ è½½æˆåŠŸï¼å…± {len(st.session_state.df)} æ¡è®°å½•ï¼Œä½†æœªèƒ½æå–å“ç‰Œå’Œå‹å·ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„", icon="âœ…")
                        
                        # å¼ºåˆ¶åˆ·æ–°é¡µé¢
                        st.rerun()
                    else:
                        st.error("æ•°æ®åŠ è½½å¤±è´¥")
                        # æ˜¾ç¤ºåŠ è½½å™¨çš„é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                        if hasattr(new_loader, 'error_message') and new_loader.error_message:
                            st.error(f"é”™è¯¯è¯¦æƒ…: {new_loader.error_message}")
                            
            except requests.exceptions.RequestException as e:
                st.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
            except Exception as e:
                st.error(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")

    if st.session_state.github_url and st.session_state.data_loaded:
        st.success(f"å·²ä»GitHubåŠ è½½æ•°æ®: {st.session_state.github_url}")
        
# ä¼šè¯ç®¡ç†
st.sidebar.header("ğŸ’¬ ä¼šè¯ç®¡ç†")
if st.sidebar.button("æ¸…é™¤èŠå¤©è®°å½•"):
    st.session_state.messages = []
    st.rerun()

# é¡µé¢å†…å®¹
if page == "èŠå¤©é¡µé¢":
    st.title("ğŸ’¬ æ•…éšœè¯Šæ–­èŠå¤©çª—å£")
    
    # æ˜¾ç¤ºèŠå¤©å†å²
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            # æ˜¾ç¤ºå›å¤å†…å®¹
            st.markdown(message["content"])
            
            # æ˜¾ç¤ºç›¸å…³æ–‡æ¡£ï¼ˆå¦‚æœæœ‰ï¼‰- æ”¾åœ¨å›å¤å†…å®¹åé¢
            if message["role"] == "assistant" and "documents" in message and message["documents"]:
                with st.expander(f"ğŸ“„ æŸ¥çœ‹ç›¸å…³æ–‡æ¡£ ({len(message['documents'])} æ¡)", expanded=False):
                    for i, doc in enumerate(message["documents"], 1):
                        st.markdown(f"**æ–‡æ¡£ {i}** (ç›¸ä¼¼åº¦: {doc.get('similarity', 0):.3f})")
                        st.markdown(f"**å‹å·**: {doc['data'].get('å‹å·', 'æœªçŸ¥')}")
                        st.markdown(f"**æŠ¥è­¦ä»£ç **: {doc['data'].get('æŠ¥è­¦ä»£ç _åŸå§‹', 'æœªçŸ¥')}")
                        st.markdown(f"**æ•…éšœç°è±¡**: {doc['data'].get('æ•…éšœç°è±¡', 'æœªçŸ¥')}")
                        st.markdown(f"**åŸå› **: {doc['data'].get('åŸå› ', 'æœªçŸ¥')}")
                        st.markdown(f"**å¤„ç†æ–¹æ³•**: {doc['data'].get('å¤„ç†æ–¹æ³•', 'æœªçŸ¥')}")
                        st.markdown(f"**æ•…éšœç±»å‹**: {doc['data'].get('æ•…éšœç±»å‹', 'æœªçŸ¥')}")
                        st.markdown("---")
    
    # èŠå¤©è¾“å…¥
    if prompt := st.chat_input("æè¿°æ‚¨é‡åˆ°çš„æ•…éšœé—®é¢˜..."):
        # æ£€æŸ¥æ˜¯å¦é€‰æ‹©äº†å‹å·
        if selected_model == "è¯·é€‰æ‹©":
            st.error("è¯·å…ˆé€‰æ‹©è®¾å¤‡å‹å·")
            st.stop()
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦åŠ è½½
        if not st.session_state.data_loaded or st.session_state.df is None or st.session_state.searcher is None:
            st.error("è¯·å…ˆä¸Šä¼ å¹¶åŠ è½½æ•…éšœæ•°æ®")
            st.stop()
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°èŠå¤©å†å²
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ç”ŸæˆåŠ©æ‰‹å›å¤
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            try:
                # ç¬¬ä¸€æ­¥ï¼šæ£€ç´¢æ–‡æ¡£
                # ä½¿ç”¨pandasè¿›è¡Œç²¾ç¡®ç­›é€‰
                pandas_results = st.session_state.loader.search_by_pandas(
                    model=selected_model,
                    alarm_code=alarm_code if alarm_code else None,
                    limit=1000  # æé«˜é™åˆ¶ï¼Œç¡®ä¿è·å–æ‰€æœ‰åŒ¹é…è®°å½•
                )
                
                # åˆå§‹åŒ– results å˜é‡
                results = []
                candidate_indices = None
                search_info = ""
                
                # è°ƒè¯•ä¿¡æ¯
                debug_info = f"æŸ¥è¯¢å‚æ•°: å‹å·={selected_model}, æŠ¥è­¦ä»£ç ={alarm_code}\n"
                debug_info += f"è§„èŒƒåŒ–æŠ¥è­¦ä»£ç : {normalize_alarm_code(alarm_code) if alarm_code else 'æ— '}\n"
                debug_info += f"PandasæŸ¥è¯¢ç»“æœè¡Œæ•°: {len(pandas_results)}\n"
                
                if not pandas_results.empty:
                    # ä½¿ç”¨DataFrameç´¢å¼•ä½œä¸ºå€™é€‰é›†
                    candidate_indices = pandas_results.index.tolist()
                    search_info = f"æ ¹æ®ç­›é€‰æ¡ä»¶æ‰¾åˆ° {len(pandas_results)} æ¡è®°å½•"
                    debug_info += f"æ‰¾åˆ°è®°å½•: {len(pandas_results)} æ¡\n"
                    
                    # å…³é”®ä¿®æ”¹ï¼šæ ¹æ®æ–‡æ¡£æ•°é‡å†³å®šæ˜¯å¦è¿›è¡Œè¯­ä¹‰æ£€ç´¢
                    if len(pandas_results) > 5:
                        search_info += "ï¼Œæ­£åœ¨è¿›è¡Œè¯­ä¹‰æœç´¢ä»¥æ‰¾åˆ°æœ€ç›¸å…³çš„ç»“æœ"
                        debug_info += "è¿›è¡Œè¯­ä¹‰æœç´¢ï¼ˆæ–‡æ¡£æ•°é‡ > 5ï¼‰\n"
                        results = st.session_state.searcher.semantic_search(
                            prompt, candidate_indices, top_k=5  # å–å‰5ä¸ªç»“æœ
                        )
                    else:
                        search_info += "ï¼Œæ–‡æ¡£æ•°é‡è¾ƒå°‘ï¼Œç›´æ¥è¿”å›æ‰€æœ‰ç»“æœ"
                        debug_info += "ç›´æ¥è¿”å›æ‰€æœ‰ç»“æœï¼ˆæ–‡æ¡£æ•°é‡ â‰¤ 5ï¼‰\n"
                        # ç›´æ¥å°†pandasç»“æœè½¬æ¢ä¸ºä¸è¯­ä¹‰æœç´¢ç›¸åŒçš„æ ¼å¼
                        for idx in candidate_indices:
                            row = st.session_state.df.loc[idx]
                            results.append({
                                'index': int(idx),
                                'similarity': 1.0,  # è®¾ç½®ä¸ºæœ€é«˜ç›¸ä¼¼åº¦
                                'data': row.to_dict()
                            })

                else:
                    # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ç»“æœ
                    if alarm_code:
                        search_info = f"æ²¡æœ‰æ‰¾åˆ°å‹å· '{selected_model}' å’ŒæŠ¥è­¦ä»£ç  '{alarm_code}' çš„åŒ¹é…è®°å½•"
                        debug_info += f"æ²¡æœ‰æ‰¾åˆ°å‹å· '{selected_model}' å’ŒæŠ¥è­¦ä»£ç  '{alarm_code}' çš„åŒ¹é…è®°å½•\n"
                    else:
                        search_info = f"æ²¡æœ‰æ‰¾åˆ°å‹å· '{selected_model}' çš„è®°å½•"
                        debug_info += f"æ²¡æœ‰æ‰¾åˆ°å‹å· '{selected_model}' çš„è®°å½•\n"
                
                # ä¿å­˜å½“å‰ç»“æœ
                st.session_state.current_results = results
                
                # #è°ƒè¯•ä¿¡æ¯
                # debug_info += f"æœ€ç»ˆç»“æœæ•°é‡: {len(results)}\n"
                # if results:
                #     debug_info += f"ç¬¬ä¸€ä¸ªç»“æœçš„æŠ¥è­¦ä»£ç : {results[0]['data'].get('æŠ¥è­¦ä»£ç _åŸå§‹', 'æœªçŸ¥')}\n"
                #     debug_info += f"ç¬¬ä¸€ä¸ªç»“æœçš„ç›¸ä¼¼åº¦: {results[0].get('similarity', 0):.3f}\n"
                
                # # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
                # st.sidebar.text_area("è°ƒè¯•ä¿¡æ¯", debug_info, height=200)
                
                # if not results:
                #     full_response = f"{search_info}\n\næŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ•…éšœä¿¡æ¯ã€‚è¯·å°è¯•æä¾›æ›´è¯¦ç»†çš„æè¿°æˆ–æ£€æŸ¥ç­›é€‰æ¡ä»¶ã€‚"
                #     message_placeholder.markdown(full_response)
                #     st.session_state.messages.append({
                #         "role": "assistant", 
                #         "content": full_response,
                #         "documents": []
                #     })
                #     st.stop()
                
                # ç¬¬äºŒæ­¥ï¼šæ˜¾ç¤ºæ–‡æ¡£ï¼ˆå›ºå®šä½ç½®ï¼Œä¸ä¼šéšæµå¼è¾“å‡ºç§»åŠ¨ï¼‰
                # ä½¿ç”¨å•ç‹¬çš„å®¹å™¨æ˜¾ç¤ºæ–‡æ¡£
                doc_container = st.container()
                with doc_container:
                    with st.expander(f"ğŸ“„ æŸ¥çœ‹ç›¸å…³æ–‡æ¡£ ({len(results)} æ¡)", expanded=False):
                        for i, result in enumerate(results, 1):
                            data = result['data']
                            st.markdown(f"**æ–‡æ¡£ {i}** (ç›¸ä¼¼åº¦: {result.get('similarity', 0):.3f})")
                            st.markdown(f"**å‹å·**: {data.get('å‹å·', 'æœªçŸ¥')}")
                            st.markdown(f"**æŠ¥è­¦ä»£ç **: {data.get('æŠ¥è­¦ä»£ç _åŸå§‹', 'æœªçŸ¥')}")
                            st.markdown(f"**æ•…éšœç°è±¡**: {data.get('æ•…éšœç°è±¡', 'æœªçŸ¥')}")
                            st.markdown(f"**åŸå› **: {data.get('åŸå› ', 'æœªçŸ¥')}")
                            st.markdown(f"**å¤„ç†æ–¹æ³•**: {data.get('å¤„ç†æ–¹æ³•', 'æœªçŸ¥')}")
                            st.markdown(f"**æ•…éšœç±»å‹**: {data.get('æ•…éšœç±»å‹', 'æœªçŸ¥')}")
                            st.markdown("---")
                
                # ç¬¬ä¸‰æ­¥ï¼šæ„å»ºæç¤ºè¯ï¼Œå°†æ–‡æ¡£å†…å®¹äº¤ç»™å¤§æ¨¡å‹
                context = "ç›¸å…³çš„æ•…éšœä¿¡æ¯ï¼š\n"
                if results:
                    for i, result in enumerate(results, 1):
                        data = result['data']
                        context += f"\n--- ç»“æœ {i} (ç›¸ä¼¼åº¦: {result['similarity']:.3f}) ---\n"
                        context += f"å‹å·: {data.get('å‹å·', 'æœªçŸ¥')}\n"
                        context += f"æŠ¥è­¦ä»£ç : {data.get('æŠ¥è­¦ä»£ç _åŸå§‹', 'æœªçŸ¥')}\n"
                        context += f"æ•…éšœç°è±¡: {data.get('æ•…éšœç°è±¡', 'æœªçŸ¥')}\n"
                        context += f"åŸå› : {data.get('åŸå› ', 'æœªçŸ¥')}\n"
                        context += f"å¤„ç†æ–¹æ³•: {data.get('å¤„ç†æ–¹æ³•', 'æœªçŸ¥')}\n"
                        context += f"æ•…éšœç±»å‹: {data.get('æ•…éšœç±»å‹', 'æœªçŸ¥')}\n"
                else:
                    context += "\næœªæ‰¾åˆ°ç›¸å…³æ•…éšœä¿¡æ¯ã€‚\n"
                
                prompt_with_context = f"""
                ç”¨æˆ·æŸ¥è¯¢: {prompt}
                
                {context}
                
                è¯·æ ¹æ®ä»¥ä¸Šæ•…éšœä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›ä¸“ä¸šçš„æ•…éšœè¯Šæ–­å’Œå¤„ç†å»ºè®®ï¼š
                1. é¦–å…ˆç¡®è®¤æ•…éšœç±»å‹å’Œå¯èƒ½çš„åŸå› ã€‚å¦‚æœæœ‰å¤šä¸ªå¯èƒ½çš„åŸå› ï¼ŒæŒ‰å¯èƒ½æ€§æ’åºè¯´æ˜ã€‚
                2. æä¾›å…·ä½“çš„å¤„ç†æ­¥éª¤å’Œæ–¹æ³•
                3. ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€å›ç­”
                
                æ³¨æ„ï¼šåªåŸºäºæä¾›çš„ä¿¡æ¯å›ç­”ï¼Œä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„ä¿¡æ¯ã€‚
                """
                
                # ç¬¬å››æ­¥ï¼šè°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›å¤
                # åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯
                client = OpenAI(
                    api_key=st.session_state.api_key,
                    base_url=st.session_state.base_url
                )
                
                # è°ƒç”¨DeepSeekç”Ÿæˆå›ç­”
                response = client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {
                            "role": "system", 
                            "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ§è®¾å¤‡æ•…éšœè¯Šæ–­ä¸“å®¶ï¼Œæ ¹æ®æä¾›çš„æ•…éšœä¿¡æ¯ç»™å‡ºå‡†ç¡®ä¸“ä¸šçš„å›ç­”ã€‚"
                        },
                        {"role": "user", "content": prompt_with_context}
                    ],
                    temperature=0.1,
                    stream=True
                )
                
                # æµå¼æ˜¾ç¤ºå›å¤
                for chunk in response:
                    if chunk.choices[0].delta.content is not None:
                        full_response += chunk.choices[0].delta.content
                        message_placeholder.markdown(full_response + "â–Œ")
                
                message_placeholder.markdown(full_response)
                
            except Exception as e:
                full_response = f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"
                message_placeholder.markdown(full_response)
                results = []  # ç¡®ä¿ results è¢«å®šä¹‰
            
            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°èŠå¤©å†å²
            st.session_state.messages.append({
                "role": "assistant", 
                "content": full_response,
                "documents": results
            })

elif page == "æ•°æ®å±•ç¤º":
    st.title("ğŸ“Š æ•°æ®å±•ç¤º")
    
    # æ£€æŸ¥æ•°æ®åŠ è½½çŠ¶æ€ - åŒæ—¶æ£€æŸ¥loaderå’ŒdfçŠ¶æ€
    if not hasattr(st.session_state, 'data_loaded') or not st.session_state.data_loaded or \
       not hasattr(st.session_state, 'df') or st.session_state.df is None:
        st.warning("è¯·å…ˆä¸Šä¼ å¹¶åŠ è½½æ•°æ®")
    else:
        st.success(f"å·²åŠ è½½ {len(st.session_state.df)} æ¡æ•…éšœè®°å½•")
        
        # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼ - åªæ˜¾ç¤ºåŸå§‹æŠ¥è­¦ä»£ç ï¼Œä¸æ˜¾ç¤ºè§„èŒƒåŒ–ç‰ˆæœ¬
        display_df = st.session_state.df.copy().reset_index(drop=True)

        if 'åºå·' in display_df.columns:
            display_df = display_df.drop('åºå·', axis=1)
        
        # ç§»é™¤è§„èŒƒåŒ–æŠ¥è­¦ä»£ç åˆ—
        if 'æŠ¥è­¦ä»£ç _è§„èŒƒåŒ–' in display_df.columns:
            display_df = display_df.drop('æŠ¥è­¦ä»£ç _è§„èŒƒåŒ–', axis=1)
        
        # é‡å‘½ååŸå§‹æŠ¥è­¦ä»£ç åˆ—ä¸ºæŠ¥è­¦ä»£ç 
        if 'æŠ¥è­¦ä»£ç _åŸå§‹' in display_df.columns:
            display_df = display_df.rename(columns={'æŠ¥è­¦ä»£ç _åŸå§‹': 'æŠ¥è­¦ä»£ç '})
        
        # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹
        for col in display_df.columns:
            display_df[col] = display_df[col].astype(str)
        
        # ä¿®å¤é‡å¤åˆ—åé—®é¢˜
        # åˆ é™¤é‡å¤çš„åˆ—ï¼ˆå¦‚æœæœ‰ï¼‰
        display_df = display_df.loc[:, ~display_df.columns.duplicated()]
        
        st.dataframe(display_df, width=1200, height=600)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        st.subheader("æ•°æ®ç»Ÿè®¡")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("æ€»è®°å½•æ•°", len(st.session_state.df))
        
        with col2:
            unique_models = st.session_state.df['å‹å·'].nunique()
            st.metric("è®¾å¤‡å‹å·æ•°", unique_models)
        
        with col3:
            # ä½¿ç”¨åŸå§‹æŠ¥è­¦ä»£ç åˆ—è¿›è¡Œç»Ÿè®¡
            if 'æŠ¥è­¦ä»£ç _åŸå§‹' in st.session_state.df.columns:
                unique_alarm_codes = st.session_state.df['æŠ¥è­¦ä»£ç _åŸå§‹'].nunique()
            else:
                unique_alarm_codes = st.session_state.df['æŠ¥è­¦ä»£ç '].nunique()
            st.metric("æŠ¥è­¦ä»£ç æ•°", str(unique_alarm_codes))
        
        # æ˜¾ç¤ºå‹å·åˆ†å¸ƒ
        st.subheader("è®¾å¤‡å‹å·åˆ†å¸ƒ")
        model_counts = st.session_state.df['å‹å·'].value_counts()
        model_counts.index = model_counts.index.astype(str)
        st.bar_chart(model_counts.head(10))
        
        # æ˜¾ç¤ºæ•…éšœç±»å‹åˆ†å¸ƒ
        st.subheader("æ•…éšœç±»å‹åˆ†å¸ƒ")
        fault_type_counts = st.session_state.df['æ•…éšœç±»å‹'].value_counts()
        fault_type_counts.index = fault_type_counts.index.astype(str)
        st.bar_chart(fault_type_counts)

elif page == "ä½¿ç”¨è¯´æ˜":
    st.title("ğŸ“– ä½¿ç”¨è¯´æ˜")
    
    st.markdown("""
    ## ç³»ç»Ÿä»‹ç»
    
    è¿™æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ§è®¾å¤‡æ•…éšœè¯Šæ–­ç³»ç»Ÿï¼Œå¯ä»¥å¸®åŠ©æ‚¨å¿«é€Ÿè¯Šæ–­å’Œè§£å†³è®¾å¤‡æ•…éšœé—®é¢˜ã€‚
    
    ## ä½¿ç”¨æ­¥éª¤
    
    1. **é€‰æ‹©è®¾å¤‡å‹å·** (å¿…éœ€)
       - åœ¨ä¾§è¾¹æ é€šè¿‡ä¸‹æ‹‰èœå•é€‰æ‹©å“ç‰Œã€äº§å“ç±»å‹å’Œå…·ä½“å‹å·
       
    2. **è¾“å…¥æŠ¥è­¦ä»£ç ** (å¯é€‰)
       - å¦‚æœæ‚¨çŸ¥é“æŠ¥è­¦ä»£ç ï¼Œå¯ä»¥åœ¨ä¾§è¾¹æ è¾“å…¥
       - ç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æŠ¥è­¦ä»£ç æ ¼å¼ï¼ˆå»é™¤å¼€å¤´å¤šä½™çš„0ï¼‰
       
    3. **ä¸Šä¼ æ•°æ®**
       - åœ¨ä¾§è¾¹æ ä¸Šä¼ åŒ…å«æ•…éšœä¿¡æ¯çš„Excelæ–‡ä»¶
       - æˆ–è€…è¾“å…¥GitHubæ–‡ä»¶åœ°å€ï¼ˆéœ€è¦æ˜¯åŸå§‹æ–‡ä»¶åœ°å€ï¼‰
       - ç‚¹å‡»"åŠ è½½æ•°æ®"æŒ‰é’®
       - ç³»ç»Ÿä¼šè‡ªåŠ¨ä»æ•°æ®ä¸­æå–å“ç‰Œå’Œå‹å·ä¿¡æ¯å¹¶æ›´æ–°ä¸‹æ‹‰é€‰é¡¹
       
    4. **æè¿°æ•…éšœ**
       - åœ¨èŠå¤©é¡µé¢æè¿°æ‚¨é‡åˆ°çš„æ•…éšœé—®é¢˜
       
    ## æœç´¢é€»è¾‘
    
    1. **åªæœ‰å‹å·**ï¼šåœ¨è¯¥å‹å·çš„æ‰€æœ‰è®°å½•ä¸­è¿›è¡Œè¯­ä¹‰æœç´¢ï¼Œå–ç›¸ä¼¼åº¦æœ€é«˜çš„å‰5æ¡
    2. **å‹å·+æŠ¥è­¦ä»£ç **ï¼šåœ¨åŒæ—¶åŒ¹é…å‹å·å’ŒæŠ¥è­¦ä»£ç çš„è®°å½•ä¸­è¿›è¡Œè¯­ä¹‰æœç´¢ï¼Œå–ç›¸ä¼¼åº¦æœ€é«˜çš„å‰5æ¡
       
    ## æ³¨æ„äº‹é¡¹
    
    - è®¾å¤‡å‹å·æ˜¯å¿…é€‰é¡¹ï¼Œå¦åˆ™æ— æ³•è¿›è¡Œæœç´¢
    - æŠ¥è­¦ä»£ç ä¼šè‡ªåŠ¨è§„èŒƒåŒ–å¤„ç†ï¼ˆå»é™¤å¼€å¤´å¤šä½™çš„0ï¼‰
    - ç¡®ä¿Excelæ–‡ä»¶åŒ…å«å¿…è¦çš„åˆ—ï¼šåºå·ã€æŠ¥è­¦ä»£ç ã€æ•…éšœç°è±¡ã€åŸå› ã€å¤„ç†æ–¹æ³•ã€æ•…éšœç±»å‹ã€å‹å·
    - å¦‚æœExcelæ–‡ä»¶ä¸­åŒ…å«å“ç‰Œå’Œäº§å“ç±»å‹åˆ—ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æå–è¿™äº›ä¿¡æ¯å¹¶æ›´æ–°ä¸‹æ‹‰é€‰é¡¹
    - GitHubæ–‡ä»¶åœ°å€éœ€è¦æ˜¯åŸå§‹æ–‡ä»¶åœ°å€ï¼ˆrawæ ¼å¼ï¼‰ï¼Œä¾‹å¦‚ï¼šhttps://github.com/ç”¨æˆ·å/é¡¹ç›®å/raw/refs/heads/main/æ–‡ä»¶å.xlsx
    """)

# è¿è¡Œè¯´æ˜
# st.sidebar.markdown("---")
# st.sidebar.info("""
# **è¿è¡Œè¯´æ˜**:
# 1. å®‰è£…ä¾èµ–: `pip install streamlit pandas openpyxl sentence-transformers scikit-learn openai`
# 2. è¿è¡Œåº”ç”¨: `streamlit run app.py`
# """)
